From 5437a5d7a11ac6e1002ce10d60e347dd40f5624c Mon Sep 17 00:00:00 2001
From: mekya <ahmetmermerkaya@gmail.com>
Date: Thu, 7 Jul 2022 10:33:35 +0300
Subject: [PATCH] Support external audio from Broadcast Extension

---
 sdk/BUILD.gn                                  |   8 +
 .../RTCAudioDeviceModule+Private.h            |  16 ++
 .../api/peerconnection/RTCAudioDeviceModule.h |  18 ++
 .../peerconnection/RTCAudioDeviceModule.mm    |  27 +++
 .../peerconnection/RTCPeerConnectionFactory.h |   5 +
 .../RTCPeerConnectionFactory.mm               |  31 ++++
 sdk/objc/native/src/audio/audio_device_ios.h  |  11 +-
 sdk/objc/native/src/audio/audio_device_ios.mm | 159 +++++++++++++++---
 .../src/audio/audio_device_module_ios.h       |   5 +
 .../src/audio/audio_device_module_ios.mm      |   5 +
 10 files changed, 259 insertions(+), 26 deletions(-)
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm

diff --git a/sdk/BUILD.gn b/sdk/BUILD.gn
index 90e245a18d..579e144380 100644
--- a/sdk/BUILD.gn
+++ b/sdk/BUILD.gn
@@ -865,6 +865,7 @@ if (is_ios || is_mac) {
         "..:no_global_constructors",
       ]
       sources = [
+        "objc/api/peerconnection/RTCAudioDeviceModule.h",
         "objc/api/peerconnection/RTCAudioSource+Private.h",
         "objc/api/peerconnection/RTCAudioSource.h",
         "objc/api/peerconnection/RTCAudioSource.mm",
@@ -1018,6 +1019,11 @@ if (is_ios || is_mac) {
       ]
 
       if (is_ios) {
+        sources += [
+          "objc/api/peerconnection/RTCAudioDeviceModule+Private.h",
+          "objc/api/peerconnection/RTCAudioDeviceModule.mm",
+        ]
+        
         deps += [ ":native_api_audio_device_module" ]
       }
     }
@@ -1256,6 +1262,7 @@ if (is_ios || is_mac) {
           "objc/helpers/RTCCameraPreviewView.h",
           "objc/helpers/RTCDispatcher.h",
           "objc/helpers/UIDevice+RTCDevice.h",
+          "objc/api/peerconnection/RTCAudioDeviceModule.h",
           "objc/api/peerconnection/RTCAudioSource.h",
           "objc/api/peerconnection/RTCAudioTrack.h",
           "objc/api/peerconnection/RTCConfiguration.h",
@@ -1367,6 +1374,7 @@ if (is_ios || is_mac) {
         output_name = "WebRTC"
 
         sources = [
+          "objc/api/peerconnection/RTCAudioDeviceModule.h",
           "objc/api/peerconnection/RTCAudioSource.h",
           "objc/api/peerconnection/RTCAudioTrack.h",
           "objc/api/peerconnection/RTCCertificate.h",
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h b/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
new file mode 100644
index 0000000000..1ef87cea10
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
@@ -0,0 +1,16 @@
+#import "RTCAudioDeviceModule.h"
+
+#if defined(WEBRTC_IOS)
+#include "sdk/objc/native/src/audio/audio_device_module_ios.h"
+
+NS_ASSUME_NONNULL_BEGIN
+
+@interface RTCAudioDeviceModule ()
+
+@property(nonatomic, readonly) rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS>
+    nativeModule;
+
+@end
+
+NS_ASSUME_NONNULL_END
+#endif
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
new file mode 100644
index 0000000000..9485ec361c
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
@@ -0,0 +1,18 @@
+
+#import <CoreMedia/CoreMedia.h>
+#import <Foundation/Foundation.h>
+
+#import "RTCMacros.h"
+
+NS_ASSUME_NONNULL_BEGIN
+
+RTC_OBJC_EXPORT
+
+NS_CLASS_AVAILABLE_IOS(2_0)
+@interface RTCAudioDeviceModule : NSObject
+
+- (void)deliverRecordedData:(CMSampleBufferRef)sampleBuffer;
+
+@end
+
+NS_ASSUME_NONNULL_END
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm
new file mode 100644
index 0000000000..531bac8eda
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm
@@ -0,0 +1,27 @@
+
+#include <AudioUnit/AudioUnit.h>
+
+#import "RTCAudioDeviceModule+Private.h"
+#include "rtc_base/ref_counted_object.h"
+
+@implementation RTCAudioDeviceModule {
+  rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS> _nativeModule;
+}
+
+- (instancetype)init {
+  self = [super init];
+  _nativeModule = new rtc::RefCountedObject<webrtc::ios_adm::AudioDeviceModuleIOS>(false);
+  return self;
+}
+
+- (void)deliverRecordedData:(CMSampleBufferRef)sampleBuffer {
+  _nativeModule->OnDeliverRecordedExternalData(sampleBuffer);
+}
+
+#pragma mark - Private
+
+- (rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS>)nativeModule {
+  return _nativeModule;
+}
+
+@end
\ No newline at end of file
diff --git a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
index 78913527c0..5adeef8f6b 100644
--- a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
+++ b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
@@ -11,6 +11,7 @@
 #import <Foundation/Foundation.h>
 
 #import "RTCMacros.h"
+#import "RTCAudioDeviceModule.h"
 
 NS_ASSUME_NONNULL_BEGIN
 
@@ -41,6 +42,10 @@ RTC_OBJC_EXPORT
     initWithEncoderFactory:(nullable id<RTC_OBJC_TYPE(RTCVideoEncoderFactory)>)encoderFactory
             decoderFactory:(nullable id<RTC_OBJC_TYPE(RTCVideoDecoderFactory)>)decoderFactory;
 
+- (instancetype)initWithEncoderFactory:(nullable id<RTCVideoEncoderFactory>)encoderFactory
+                                    decoderFactory:(nullable id<RTCVideoDecoderFactory>)decoderFactory
+                                 audioDeviceModule:(RTCAudioDeviceModule *)audioDeviceModule NS_AVAILABLE_IOS(2_0);
+
 /** Initialize an RTCAudioSource with constraints. */
 - (RTC_OBJC_TYPE(RTCAudioSource) *)audioSourceWithConstraints:
     (nullable RTC_OBJC_TYPE(RTCMediaConstraints) *)constraints;
diff --git a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
index 1f0549ae8a..5675d777ba 100644
--- a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
+++ b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
@@ -14,6 +14,7 @@
 #import "RTCPeerConnectionFactory+Private.h"
 #import "RTCPeerConnectionFactoryOptions+Private.h"
 
+#import "RTCAudioDeviceModule+Private.h"
 #import "RTCAudioSource+Private.h"
 #import "RTCAudioTrack+Private.h"
 #import "RTCMediaConstraints+Private.h"
@@ -113,6 +114,36 @@ - (instancetype)init {
                            audioProcessingModule:nullptr];
 #endif
 }
+            
+
+#if defined(WEBRTC_IOS)
+- (instancetype)initWithEncoderFactory:(nullable id<RTCVideoEncoderFactory>)encoderFactory
+                        decoderFactory:(nullable id<RTCVideoDecoderFactory>)decoderFactory
+                     audioDeviceModule:(RTCAudioDeviceModule *)audioDeviceModule {
+	#ifdef HAVE_NO_MEDIA
+	  return [self initWithNoMedia];
+	#else
+	  std::unique_ptr<webrtc::VideoEncoderFactory> native_encoder_factory;
+	  std::unique_ptr<webrtc::VideoDecoderFactory> native_decoder_factory;
+	  if (encoderFactory) {
+		native_encoder_factory = webrtc::ObjCToNativeVideoEncoderFactory(encoderFactory);
+	  }
+	  if (decoderFactory) {
+		native_decoder_factory = webrtc::ObjCToNativeVideoDecoderFactory(decoderFactory);
+	  }
+	  return [self initWithNativeAudioEncoderFactory:webrtc::CreateBuiltinAudioEncoderFactory()
+						   nativeAudioDecoderFactory:webrtc::CreateBuiltinAudioDecoderFactory()
+						   nativeVideoEncoderFactory:std::move(native_encoder_factory)
+						   nativeVideoDecoderFactory:std::move(native_decoder_factory)
+								   audioDeviceModule:audioDeviceModule.nativeModule
+							   audioProcessingModule:nullptr];
+	#endif
+}
+#endif            
+            
+            
+            
+            
 - (instancetype)initNative {
   if (self = [super init]) {
     _networkThread = rtc::Thread::CreateWithSocketServer();
diff --git a/sdk/objc/native/src/audio/audio_device_ios.h b/sdk/objc/native/src/audio/audio_device_ios.h
index 5afc49a461..6079b6d693 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_ios.h
@@ -11,6 +11,7 @@
 #ifndef SDK_OBJC_NATIVE_SRC_AUDIO_AUDIO_DEVICE_IOS_H_
 #define SDK_OBJC_NATIVE_SRC_AUDIO_AUDIO_DEVICE_IOS_H_
 
+#include <CoreMedia/CoreMedia.h>
 #include <memory>
 
 #include "api/sequence_checker.h"
@@ -146,6 +147,9 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   void OnCanPlayOrRecordChange(bool can_play_or_record) override;
   void OnChangedOutputVolume() override;
 
+  void OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer);
+
+
   // VoiceProcessingAudioUnitObserver methods.
   OSStatus OnDeliverRecordedData(AudioUnitRenderActionFlags* flags,
                                  const AudioTimeStamp* time_stamp,
@@ -217,7 +221,10 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   SequenceChecker thread_checker_;
 
   // Native I/O audio thread checker.
-  SequenceChecker io_thread_checker_;
+  //SequenceChecker io_thread_checker_;
+
+  // Native audio I/O mutex.
+  Mutex io_mutex_;
 
   // Thread that this object is created on.
   rtc::Thread* thread_;
@@ -290,7 +297,7 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
 
   // Counts number of detected audio glitches on the playout side.
   int64_t num_detected_playout_glitches_ RTC_GUARDED_BY(thread_checker_);
-  int64_t last_playout_time_ RTC_GUARDED_BY(io_thread_checker_);
+  int64_t last_playout_time_ RTC_GUARDED_BY(io_mutex_);
 
   // Counts number of playout callbacks per call.
   // The value isupdated on the native I/O thread and later read on the
diff --git a/sdk/objc/native/src/audio/audio_device_ios.mm b/sdk/objc/native/src/audio/audio_device_ios.mm
index 3ec7d0b75a..afa1129c3f 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_ios.mm
@@ -11,6 +11,7 @@
 #import <AVFoundation/AVFoundation.h>
 #import <Foundation/Foundation.h>
 
+#import <CoreMedia/CoreMedia.h>
 #include "audio_device_ios.h"
 
 #include <cmath>
@@ -115,7 +116,7 @@ static void LogDeviceInfo() {
       last_output_volume_change_time_(0) {
   LOGI() << "ctor" << ios::GetCurrentThreadDescription()
          << ",bypass_voice_processing=" << bypass_voice_processing_;
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
   thread_checker_.Detach();
   thread_ = rtc::Thread::Current();
 
@@ -139,8 +140,8 @@ static void LogDeviceInfo() {
 
 AudioDeviceGeneric::InitStatus AudioDeviceIOS::Init() {
   LOGI() << "Init";
-  io_thread_checker_.Detach();
-  thread_checker_.Detach();
+  //io_thread_checker_.Detach();
+  //thread_checker_.Detach();
 
   RTC_DCHECK_RUN_ON(&thread_checker_);
   if (initialized_) {
@@ -149,6 +150,7 @@ static void LogDeviceInfo() {
 #if !defined(NDEBUG)
   LogDeviceInfo();
 #endif
+  /*
   // Store the preferred sample rate and preferred number of channels already
   // here. They have not been set and confirmed yet since configureForWebRTC
   // is not called until audio is about to start. However, it makes sense to
@@ -162,6 +164,7 @@ static void LogDeviceInfo() {
   // we will always tell the I/O audio unit to do a channel format conversion
   // to guarantee mono on the "input side" of the audio unit.
   UpdateAudioDeviceBuffer();
+  */
   initialized_ = true;
   return InitStatus::OK;
 }
@@ -231,6 +234,12 @@ static void LogDeviceInfo() {
   RTC_DCHECK(audio_is_initialized_);
   RTC_DCHECK(!playing_);
   RTC_DCHECK(audio_unit_);
+  
+  if (!audio_unit_) {
+     RTCLogError(@"StartPlayout failed because audio is disabled.");
+     return -1;
+  }
+  
   if (fine_audio_buffer_) {
     fine_audio_buffer_->ResetPlayout();
   }
@@ -286,11 +295,11 @@ static void LogDeviceInfo() {
   RTC_DCHECK_RUN_ON(&thread_checker_);
   RTC_DCHECK(audio_is_initialized_);
   RTC_DCHECK(!recording_);
-  RTC_DCHECK(audio_unit_);
+  //RTC_DCHECK(audio_unit_);
   if (fine_audio_buffer_) {
     fine_audio_buffer_->ResetRecord();
   }
-  if (!playing_ && audio_unit_->GetState() == VoiceProcessingAudioUnit::kInitialized) {
+  if (!playing_ && audio_unit_ && audio_unit_->GetState() == VoiceProcessingAudioUnit::kInitialized) {
     OSStatus result = audio_unit_->Start();
     if (result != noErr) {
       RTC_OBJC_TYPE(RTCAudioSession)* session = [RTC_OBJC_TYPE(RTCAudioSession) sharedInstance];
@@ -311,7 +320,10 @@ static void LogDeviceInfo() {
     return 0;
   }
   if (!playing_) {
-    ShutdownPlayOrRecord();
+   
+    if (audio_unit_) {
+         ShutdownPlayOrRecord();
+    }
     audio_is_initialized_ = false;
   }
   rtc::AtomicOps::ReleaseStore(&recording_, 0);
@@ -373,12 +385,72 @@ static void LogDeviceInfo() {
   thread_->Post(RTC_FROM_HERE, this, kMessageOutputVolumeChange);
 }
 
+void AudioDeviceIOS::OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer) {
+  MutexLock scoped_lock(&io_mutex_);
+
+  if (audio_unit_ && audio_unit_->GetState() != VoiceProcessingAudioUnit::kUninitialized) {
+    RTCLogError(@"External recorded data was provided while audio unit is enabled.");
+    return;
+  }
+
+  CMFormatDescriptionRef description = CMSampleBufferGetFormatDescription(sample_buffer);
+  const AudioStreamBasicDescription *asbd = CMAudioFormatDescriptionGetStreamBasicDescription(description);
+  if (!asbd) {
+    RTCLogError(@"External recorded data was not in audio format.");
+    return;
+  }
+
+  if (asbd->mSampleRate != record_parameters_.sample_rate() ||
+      asbd->mChannelsPerFrame != record_parameters_.channels()) {
+    record_parameters_.reset(asbd->mSampleRate, asbd->mChannelsPerFrame);
+    UpdateAudioDeviceBuffer();
+
+    // Create a modified audio buffer class which allows us to ask for,
+    // or deliver, any number of samples (and not only multiple of 10ms) to match
+    // the native audio unit buffer size.
+    RTC_DCHECK(audio_device_buffer_);
+    fine_audio_buffer_.reset(new FineAudioBuffer(audio_device_buffer_));
+  }
+
+  CMBlockBufferRef block_buffer = CMSampleBufferGetDataBuffer(sample_buffer);
+  if (block_buffer == nil) {
+    return;
+  }
+
+  AudioBufferList buffer_list;
+  CMSampleBufferGetAudioBufferListWithRetainedBlockBuffer(sample_buffer,
+                                                          nullptr,
+                                                          &buffer_list,
+                                                          sizeof(buffer_list),
+                                                          nullptr,
+                                                          nullptr,
+                                                          kCMSampleBufferFlag_AudioBufferList_Assure16ByteAlignment,
+                                                          &block_buffer);
+
+  rtc::ArrayView<int16_t> view {
+    static_cast<int16_t*>(buffer_list.mBuffers[0].mData),
+    buffer_list.mBuffers[0].mDataByteSize / sizeof(int16_t)
+  };
+
+  if (asbd->mFormatFlags & kAudioFormatFlagIsBigEndian) {
+    for (auto& element : view) {
+      element = be16toh(element);
+    }
+  }
+
+  fine_audio_buffer_->DeliverRecordedData(view, kFixedRecordDelayEstimate);
+
+  CFRelease(block_buffer);
+}
+
 OSStatus AudioDeviceIOS::OnDeliverRecordedData(AudioUnitRenderActionFlags* flags,
                                                const AudioTimeStamp* time_stamp,
                                                UInt32 bus_number,
                                                UInt32 num_frames,
                                                AudioBufferList* /* io_data */) {
-  RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  //RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  MutexLock scoped_lock(&io_mutex_);
+
   OSStatus result = noErr;
   // Simply return if recording is not enabled.
   if (!rtc::AtomicOps::AcquireLoad(&recording_)) return result;
@@ -426,8 +498,9 @@ static void LogDeviceInfo() {
                                           UInt32 bus_number,
                                           UInt32 num_frames,
                                           AudioBufferList* io_data) {
-  RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  //RTC_DCHECK_RUN_ON(&io_thread_checker_);
   // Verify 16-bit, noninterleaved mono PCM signal format.
+  MutexLock scoped_lock(&io_mutex_);
   RTC_DCHECK_EQ(1, io_data->mNumberBuffers);
   AudioBuffer* audio_buffer = &io_data->mBuffers[0];
   RTC_DCHECK_EQ(1, audio_buffer->mNumberChannels);
@@ -678,15 +751,26 @@ static void LogDeviceInfo() {
   // AttachAudioBuffer() is called at construction by the main class but check
   // just in case.
   RTC_DCHECK(audio_device_buffer_) << "AttachAudioBuffer must be called first";
-  RTC_DCHECK_GT(playout_parameters_.sample_rate(), 0);
-  RTC_DCHECK_GT(record_parameters_.sample_rate(), 0);
-  RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
-  RTC_DCHECK_EQ(record_parameters_.channels(), 1);
+  //RTC_DCHECK_GT(playout_parameters_.sample_rate(), 0);
+  //RTC_DCHECK_GT(record_parameters_.sample_rate(), 0);
+  //RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
+  //RTC_DCHECK_EQ(record_parameters_.channels(), 1);
   // Inform the audio device buffer (ADB) about the new audio format.
-  audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
-  audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
-  audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
-  audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+  //audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
+  //audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
+  //audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
+  //audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+  
+  if (playout_parameters_.is_valid()) {
+     RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
+     audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
+     audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
+   }
+   if (record_parameters_.is_valid()) {
+     RTC_DCHECK_EQ(record_parameters_.channels(), 1);
+     audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
+     audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+   }
 }
 
 void AudioDeviceIOS::SetupAudioBuffersForActiveAudioSession() {
@@ -723,9 +807,9 @@ static void LogDeviceInfo() {
   // number of audio frames.
   // Example: IO buffer size = 0.008 seconds <=> 128 audio frames at 16kHz.
   // Hence, 128 is the size we expect to see in upcoming render callbacks.
-  playout_parameters_.reset(sample_rate, playout_parameters_.channels(), io_buffer_duration);
+  playout_parameters_.reset(sample_rate, webRTCConfig.outputNumberOfChannels, io_buffer_duration);
   RTC_DCHECK(playout_parameters_.is_complete());
-  record_parameters_.reset(sample_rate, record_parameters_.channels(), io_buffer_duration);
+  record_parameters_.reset(sample_rate, webRTCConfig.inputNumberOfChannels, io_buffer_duration);
   RTC_DCHECK(record_parameters_.is_complete());
   RTC_LOG(LS_INFO) << " frames per I/O buffer: " << playout_parameters_.frames_per_buffer();
   RTC_LOG(LS_INFO) << " bytes per I/O buffer: " << playout_parameters_.GetBytesPerBuffer();
@@ -769,7 +853,12 @@ static void LogDeviceInfo() {
   if (!audio_is_initialized_) return;
 
   // If we're initialized, we must have an audio unit.
-  RTC_DCHECK(audio_unit_);
+  //RTC_DCHECK(audio_unit_);
+  
+  if (can_play_or_record && !audio_unit_ && !CreateAudioUnit()) {
+     RTCLog(@"Failed to create audio unit.");
+     return;
+   }
 
   bool should_initialize_audio_unit = false;
   bool should_uninitialize_audio_unit = false;
@@ -898,9 +987,9 @@ static void LogDeviceInfo() {
   RTC_DCHECK_RUN_ON(&thread_checker_);
 
   // There should be no audio unit at this point.
-  if (!CreateAudioUnit()) {
-    return false;
-  }
+  //if (!CreateAudioUnit()) {
+  //  return false;
+  //}
 
   RTC_OBJC_TYPE(RTCAudioSession)* session = [RTC_OBJC_TYPE(RTCAudioSession) sharedInstance];
   // Subscribe to audio session events.
@@ -920,6 +1009,28 @@ static void LogDeviceInfo() {
   // If we are ready to play or record, and if the audio session can be
   // configured, then initialize the audio unit.
   if (session.canPlayOrRecord) {
+	   // Store the preferred sample rate and preferred number of channels already
+	    // here. They have not been set and confirmed yet since configureForWebRTC
+	    // is not called until audio is about to start. However, it makes sense to
+	    // store the parameters now and then verify at a later stage.
+	    RTC_OBJC_TYPE(RTCAudioSessionConfiguration)* config =
+	        [RTC_OBJC_TYPE(RTCAudioSessionConfiguration) webRTCConfiguration];
+	    playout_parameters_.reset(config.sampleRate, config.outputNumberOfChannels);
+	    record_parameters_.reset(config.sampleRate, config.inputNumberOfChannels);
+	    // Ensure that the audio device buffer (ADB) knows about the internal audio
+	    // parameters. Note that, even if we are unable to get a mono audio session,
+	    // we will always tell the I/O audio unit to do a channel format conversion
+	    // to guarantee mono on the "input side" of the audio unit.
+	    UpdateAudioDeviceBuffer();
+
+	    // There should be no audio unit at this point.
+	    if (!CreateAudioUnit()) {
+	      [session unlockForConfiguration];
+	      return false;
+	    }  
+	  
+	  
+	  
     if (!ConfigureAudioSessionLocked()) {
       // One possible reason for failure is if an attempt was made to use the
       // audio session during or after a Media Services failure.
@@ -949,7 +1060,7 @@ static void LogDeviceInfo() {
 
   // Detach thread checker for the AURemoteIO::IOThread to ensure that the
   // next session uses a fresh thread id.
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
 
   // Remove audio session notification observers.
   RTC_OBJC_TYPE(RTCAudioSession)* session = [RTC_OBJC_TYPE(RTCAudioSession) sharedInstance];
@@ -966,7 +1077,7 @@ static void LogDeviceInfo() {
   // restart. It will result in audio callbacks from a new native I/O thread
   // which means that we must detach thread checkers here to be prepared for an
   // upcoming new audio stream.
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
 }
 
 bool AudioDeviceIOS::IsInterrupted() {
diff --git a/sdk/objc/native/src/audio/audio_device_module_ios.h b/sdk/objc/native/src/audio/audio_device_module_ios.h
index 9bcf114e32..7833864de2 100644
--- a/sdk/objc/native/src/audio/audio_device_module_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_module_ios.h
@@ -20,6 +20,10 @@
 #include "modules/audio_device/include/audio_device.h"
 #include "rtc_base/checks.h"
 
+#if defined(WEBRTC_IOS)
+#include <CoreMedia/CoreMedia.h>
+#endif
+
 namespace webrtc {
 
 class AudioDeviceGeneric;
@@ -127,6 +131,7 @@ class AudioDeviceModuleIOS : public AudioDeviceModule {
   int32_t GetPlayoutUnderrunCount() const override;
 
 #if defined(WEBRTC_IOS)
+  void OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer);
   int GetPlayoutAudioParameters(AudioParameters* params) const override;
   int GetRecordAudioParameters(AudioParameters* params) const override;
 #endif  // WEBRTC_IOS
diff --git a/sdk/objc/native/src/audio/audio_device_module_ios.mm b/sdk/objc/native/src/audio/audio_device_module_ios.mm
index 859442dc9e..b956b9b23b 100644
--- a/sdk/objc/native/src/audio/audio_device_module_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_module_ios.mm
@@ -651,6 +651,11 @@
   }
 
 #if defined(WEBRTC_IOS)
+  
+  void AudioDeviceModuleIOS::OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer) {
+    audio_device_->OnDeliverRecordedExternalData(sample_buffer);
+  }
+  
   int AudioDeviceModuleIOS::GetPlayoutAudioParameters(
       AudioParameters* params) const {
     RTC_DLOG(INFO) << __FUNCTION__;
-- 
2.37.1 (Apple Git-137.1)

